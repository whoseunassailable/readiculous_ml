{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710706c4",
   "metadata": {},
   "source": [
    "## ðŸ§© Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccafddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5447725",
   "metadata": {},
   "source": [
    "## ðŸ“ Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2afa6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "raw_dir = os.path.join(data_dir, 'raw')\n",
    "processed_dir = os.path.join(data_dir, 'processed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa94132",
   "metadata": {},
   "source": [
    "## ðŸ“š Load Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea296138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data():\n",
    "    print(\"Loading raw datasets...\")\n",
    "\n",
    "    books_path = os.path.join(raw_dir, '/Users/whoseunassailable/Documents/coding_projects/college_projects/readiculous/data/raw/Books.csv')\n",
    "    ratings_path = os.path.join(raw_dir, '/Users/whoseunassailable/Documents/coding_projects/college_projects/readiculous/data/raw/Ratings.csv')\n",
    "    users_path = os.path.join(raw_dir, '/Users/whoseunassailable/Documents/coding_projects/college_projects/readiculous/data/raw/Users.csv')\n",
    "\n",
    "    books_df = pd.read_csv(books_path, encoding='latin-1')\n",
    "    ratings_df = pd.read_csv(ratings_path, encoding='latin-1')\n",
    "    users_df = pd.read_csv(users_path, encoding='latin-1')\n",
    "\n",
    "    print(f\"Loaded {len(books_df)} books\")\n",
    "    print(f\"Loaded {len(ratings_df)} ratings\")\n",
    "    print(f\"Loaded {len(users_df)} users\")\n",
    "\n",
    "    return books_df, ratings_df, users_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63454bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_genre_string(self, genre_str):\n",
    "    if pd.isnull(genre_str) or genre_str == '[]':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Try to parse as a literal Python list\n",
    "        genres = ast.literal_eval(genre_str)\n",
    "        if isinstance(genres, list):\n",
    "            # Clean up each genre\n",
    "            return [g.strip(\"'\") for g in genres if g.strip(\"'\")]\n",
    "        return []\n",
    "    except (ValueError, SyntaxError):\n",
    "        # If there's an error, try regex matching\n",
    "        matches = re.findall(r\"'([^']*)'\", genre_str)\n",
    "        return [m for m in matches if m]\n",
    "    \n",
    "\n",
    "def load_processed_data(filename='combined_books.csv'):\n",
    "    combined_path = os.path.join(processed_dir, filename)\n",
    "    \n",
    "    if not os.path.exists(combined_path):\n",
    "        print(f\"Error: {combined_path} not found\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(combined_path)\n",
    "\n",
    "    # Process genre column if it exists\n",
    "    if 'matched_genres' in df.columns:\n",
    "        df['genres_list'] = df['matched_genres'].apply(\n",
    "            lambda x: _process_genre_string(x) if pd.notnull(x) else []\n",
    "        )\n",
    "\n",
    "    print(f\"Loaded {len(df)} processed records\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c193feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(df):\n",
    "    stats = {\n",
    "        'total_entries': len(df),\n",
    "        'unique_users': df['user_id'].nunique(),\n",
    "        'unique_books': df['isbn'].nunique(),\n",
    "        'rating_distribution': df['rating'].value_counts().to_dict(),\n",
    "        'avg_rating': df['rating'].mean(),\n",
    "        'missing_values': df.isnull().sum().to_dict()\n",
    "    }\n",
    "\n",
    "    if 'genres_list' in df.columns:\n",
    "        all_genres = []\n",
    "        for genres in df['genres_list'].dropna():\n",
    "            all_genres.extend(genres)\n",
    "        \n",
    "        genre_counts = pd.Series(all_genres).value_counts().to_dict()\n",
    "        stats['top_genres'] = dict(sorted(genre_counts.items(), key=lambda item: item[1], reverse=True)[:20])\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c543c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(self):\n",
    "    if self.books_df is None or self.ratings_df is None or self.users_df is None:\n",
    "        self.load_raw_data()\n",
    "    \n",
    "    print(\"Merging datasets...\")\n",
    "    \n",
    "    # Merge ratings with books\n",
    "    merged_df = pd.merge(self.ratings_df, self.books_df, on='ISBN', how='inner')\n",
    "    \n",
    "    # Merge with users\n",
    "    self.combined_df = pd.merge(merged_df, self.users_df, on='User-ID', how='inner')\n",
    "    \n",
    "    # Rename columns for clarity and consistency\n",
    "    self.combined_df = self.combined_df.rename(columns={\n",
    "        'User-ID': 'user_id',\n",
    "        'ISBN': 'isbn',\n",
    "        'Book-Rating': 'rating',\n",
    "        'Book-Title': 'title',\n",
    "        'Book-Author': 'author',\n",
    "        'Year-Of-Publication': 'year',\n",
    "        'Publisher': 'publisher',\n",
    "        'Age': 'age',\n",
    "        'Location': 'location'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "856d70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_combined_data(self, filename='combined_books.csv'):\n",
    "    if self.combined_df is None:\n",
    "        print(\"Error: No combined data to save. Run merge_datasets() first.\")\n",
    "        return False\n",
    "    \n",
    "    save_path = os.path.join(self.processed_dir, filename)\n",
    "    try:\n",
    "        self.combined_df.to_csv(save_path, index=False)\n",
    "        print(f\"Combined dataset saved to {save_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving combined dataset: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eab3761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw datasets...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/Books.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step-by-step run\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m books_df, ratings_df, users_df \u001b[38;5;241m=\u001b[39m load_raw_data()\n\u001b[1;32m      3\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m merge_datasets(books_df, ratings_df, users_df)\n\u001b[1;32m      4\u001b[0m save_combined_data(combined_df)\n",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m, in \u001b[0;36mload_raw_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ratings_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRatings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m users_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(raw_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m books_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(books_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m ratings_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(ratings_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m users_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(users_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/Books.csv'"
     ]
    }
   ],
   "source": [
    "# Step-by-step run\n",
    "books_df, ratings_df, users_df = load_raw_data()\n",
    "combined_df = merge_datasets(books_df, ratings_df, users_df)\n",
    "save_combined_data(combined_df)\n",
    "# OR\n",
    "# combined_df = load_processed_data()\n",
    "\n",
    "# Show statistics\n",
    "stats = get_stats(combined_df)\n",
    "print(f\"Total entries: {stats['total_entries']}\")\n",
    "print(f\"Unique users: {stats['unique_users']}\")\n",
    "print(f\"Unique books: {stats['unique_books']}\")\n",
    "print(f\"Average rating: {stats['avg_rating']:.2f}\")\n",
    "\n",
    "if 'top_genres' in stats:\n",
    "    print(\"\\nTop 10 genres:\")\n",
    "    for genre, count in list(stats['top_genres'].items())[:10]:\n",
    "        print(f\"  {genre}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa011b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class DataLoader:\n",
    "#     def __init__(self, data_dir='../data'):\n",
    "#         self.data_dir = data_dir\n",
    "#         self.raw_dir = os.path.join(data_dir, 'raw')\n",
    "#         self.processed_dir = os.path.join(data_dir, 'processed')\n",
    "        \n",
    "#         # Initialize dataframes\n",
    "#         self.books_df = None\n",
    "#         self.ratings_df = None\n",
    "#         self.users_df = None\n",
    "#         self.combined_df = None\n",
    "\n",
    "#     def load_raw_data(self):\n",
    "#         print(\"Loading raw datasets...\")\n",
    "        \n",
    "#         # Load the books dataset\n",
    "#         books_path = os.path.join(self.raw_dir, 'Books.csv')\n",
    "#         self.books_df = pd.read_csv(books_path, error_bad_lines=False, encoding='latin-1')\n",
    "#         print(f\"Loaded {len(self.books_df)} books\")\n",
    "        \n",
    "#         # Load the ratings dataset\n",
    "#         ratings_path = os.path.join(self.raw_dir, 'Ratings.csv')\n",
    "#         self.ratings_df = pd.read_csv(ratings_path, error_bad_lines=False, encoding='latin-1')\n",
    "#         print(f\"Loaded {len(self.ratings_df)} ratings\")\n",
    "        \n",
    "#         # Load the users dataset\n",
    "#         users_path = os.path.join(self.raw_dir, 'Users.csv')\n",
    "#         self.users_df = pd.read_csv(users_path, error_bad_lines=False, encoding='latin-1')\n",
    "#         print(f\"Loaded {len(self.users_df)} users\")\n",
    "        \n",
    "#         return self.books_df, self.ratings_df, self.users_df\n",
    "\n",
    "#     def load_processed_data(self, filename='combined_books.csv'):\n",
    "#         combined_path = os.path.join(self.processed_dir, filename)\n",
    "        \n",
    "#         if not os.path.exists(combined_path):\n",
    "#             print(f\"Error: Processed file {combined_path} does not exist.\")\n",
    "#             return None\n",
    "        \n",
    "#         print(f\"Loading processed dataset: {filename}\")\n",
    "#         self.combined_df = pd.read_csv(combined_path)\n",
    "        \n",
    "#         # Process genre strings into lists\n",
    "#         if 'matched_genres' in self.combined_df.columns:\n",
    "#             self.combined_df['genres_list'] = self.combined_df['matched_genres'].apply(\n",
    "#                 lambda x: self._process_genre_string(x) if pd.notnull(x) else []\n",
    "#             )\n",
    "        \n",
    "#         print(f\"Loaded {len(self.combined_df)} entries from processed data\")\n",
    "#         return self.combined_df\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "#         print(f\"Created combined dataset with {len(self.combined_df)} entries\")\n",
    "#         return self.combined_df\n",
    "    \n",
    "\n",
    "\n",
    "#     def get_stats(self):\n",
    "#         if self.combined_df is None:\n",
    "#             print(\"No data loaded. Please load data first.\")\n",
    "#             return None\n",
    "        \n",
    "#         stats = {\n",
    "#             'total_entries': len(self.combined_df),\n",
    "#             'unique_users': self.combined_df['user_id'].nunique(),\n",
    "#             'unique_books': self.combined_df['isbn'].nunique(),\n",
    "#             'rating_distribution': self.combined_df['rating'].value_counts().to_dict(),\n",
    "#             'avg_rating': self.combined_df['rating'].mean(),\n",
    "#             'missing_values': self.combined_df.isnull().sum().to_dict()\n",
    "#         }\n",
    "        \n",
    "#         # Get genre statistics if available\n",
    "#         if 'genres_list' in self.combined_df.columns:\n",
    "#             all_genres = []\n",
    "#             for genres in self.combined_df['genres_list'].dropna():\n",
    "#                 all_genres.extend(genres)\n",
    "            \n",
    "#             genre_counts = pd.Series(all_genres).value_counts().to_dict()\n",
    "#             stats['top_genres'] = dict(sorted(genre_counts.items(), \n",
    "#                                       key=lambda item: item[1], \n",
    "#                                       reverse=True)[:20])\n",
    "        \n",
    "#         return stats\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     loader = DataLoader()\n",
    "    \n",
    "#     # Either load raw data and merge\n",
    "#     # books_df, ratings_df, users_df = loader.load_raw_data()\n",
    "#     # combined_df = loader.merge_datasets()\n",
    "#     # loader.save_combined_data()\n",
    "    \n",
    "#     # Or load existing processed data\n",
    "#     combined_df = loader.load_processed_data()\n",
    "    \n",
    "#     # Print dataset statistics\n",
    "#     stats = loader.get_stats()\n",
    "#     if stats:\n",
    "#         print(\"\\nDataset Statistics:\")\n",
    "#         print(f\"Total entries: {stats['total_entries']}\")\n",
    "#         print(f\"Unique users: {stats['unique_users']}\")\n",
    "#         print(f\"Unique books: {stats['unique_books']}\")\n",
    "#         print(f\"Average rating: {stats['avg_rating']:.2f}\")\n",
    "        \n",
    "#         if 'top_genres' in stats:\n",
    "#             print(\"\\nTop 10 genres:\")\n",
    "#             for genre, count in list(stats['top_genres'].items())[:10]:\n",
    "#                 print(f\"  {genre}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
